from typing import List
from rouge_score import rouge_scorer
from gspread_dataframe import get_as_dataframe, set_with_dataframe
import pandas as pd
import gspread
from oauth2client.service_account import ServiceAccountCredentials

scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]

import streamlit as st
credentials = ServiceAccountCredentials.from_json_keyfile_dict(
    st.secrets["google_service_account"], scope
)
client = gspread.authorize(credentials)

spreadsheet = client.open("Financial Title Generator")
worksheet = spreadsheet.worksheet("Titles")


def get_llm_response(prompt: str, model: str, api_key: str) -> str:
    """
    Get a response from the LLM (Language Model) based on the provided prompt.
    
    Args:
        prompt (str): The prompt to send to the LLM.
        model (str): The model to use for generating the response.
        
    Returns:
        str: The response from the LLM.
    """
    print(f"Getting LLM response for prompt: {prompt} with api_key: {api_key}")
    # TODO: Placeholder for actual LLM response logic
    return "LLM Response"

    # client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    # try:
    #     response = client.chat.completions.create(
    #         model=model
    #         messages=[{"role": "user", "content": prompt}]
    #     )
    #     return response.choices[0].message.content
    # except Exception as e:
    #     return f"Error calling LLM: {e}"


def make_prompt_for_llm(summary: str) -> str:
    """
    Create a prompt for the LLM based on the provided summary.
    
    Args:
        summary (str): The summary to include in the prompt.
        
    Returns:
        str: The generated prompt.
    """
    print(f"Creating prompt with summary")
    # TODO: Placeholder for actual prompt creation logic
    prompt = ""
    return prompt


def clean_response(response: str) -> str:
    """
    Clean the response from the LLM to extract the relevant information.
    
    Args:
        response (str): The response from the LLM.
        
    Returns:
        str: The cleaned response.
    """
    print(f"Cleaning response")
    # TODO: Placeholder for actual response cleaning logic
    generate_titles = ["Generated Title 1", "Generated Title 2", "Generated Title 3", "Generated Title 4", "Generated Title 5"]
    return generate_titles


def generate_titles(summary: str, model: str, api_key:str) -> List[str]:
    """
    Generate titles based on the provided summary using the specified model.
    
    Args:
        summary (str): The summary text to generate titles for.
        model (str): The model to use for title generation.
        
    Returns:
        List[str]: A list of generated titles.
    """
    print(f"Generating titles for summary: {summary} using model: {model}")
    prompt = make_prompt_for_llm(summary=summary)
    response = get_llm_response(prompt=prompt, model=model, api_key=api_key)
    generated_titles = clean_response(response)
    return generated_titles


def calculate_rouge_scores(title: str, generated_title: str) -> float:
    """
    Calculate the ROUGE score between the original title and the generated title.
    
    Args:
        title (str): The title given by the user.
        generated_title (str): The title generated by the model.
        
    Returns:
        List[str]: A list of generated titles.
    """
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
    scores = scorer.score(title, generated_title)
    score = round(scores['rouge1'].fmeasure, 3)
    return (score)


def generate_titles_with_rogue_scores(summary: str, model: str, user_title: str, api_key:str) -> List[str]:
    """
    Generate titles based on the provided summary using the specified model.
    
    Args:
        summary (str): The summary text to generate titles for.
        model (str): The model to use for title generation.
        
    Returns:
        List[str]: A list of generated titles.
    """
    generated_titles= generate_titles(summary=summary, model=model, api_key=api_key)
    generated_titles_with_rouge_scores = []
    for generated_title in generated_titles:
        rouge_score = calculate_rouge_scores(title=user_title, generated_title=generated_title)
        generated_titles_with_rouge_scores.append(f"{generated_title} \t- ROUGE Score: {rouge_score}")
    print(f"Generated titles with ROUGE scores")
    return generated_titles_with_rouge_scores


def save_to_database(data: List) -> str:
    """
    Save the generated titles and their ROUGE scores to a database.
    
    Args:
        connection (str): The database connection string.
        data (List[dict]): The data to save to the database.
        
    Returns:
        str: A message indicating the result of the operation.
    """
    print(f"Saving data to database: {data}")
    try:
        df = read_from_database()
        if not df.empty:
            df["ID"] = pd.to_numeric(df["ID"], errors="coerce")
            new_id = df["ID"].max() + 1
        else:
            new_id = 1
            
        selected_title = data[3] if len(data) == 4 else ""
        if " \t- ROUGE Score: " in selected_title:
            title, score = selected_title.split(" \t- ROUGE Score: ", 1)
            data[3] = title.strip()
            data.append(score.strip())
        else:
            data.append("")            
            
        data.insert(0, new_id)
        worksheet.append_row(data)
        return True
    except Exception as e:
        print(f"Error saving data to database: {e}")
        return False


def read_from_database() -> pd.DataFrame:
    """
    Read data from the database.
    
    Args:
        connection (str): The database connection string.
        
    Returns:
        pd.DataFrame: The data read from the database.
    """
    print(f"Reading data from spreadsheets")
    df = get_as_dataframe(worksheet, evaluate_formulas=True).dropna(how="all")
    return df